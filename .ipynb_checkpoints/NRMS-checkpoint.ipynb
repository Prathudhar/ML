{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Gd_s728FcVH",
    "outputId": "8082e4db-196a-4b08-d80d-9e4a0606ae48"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import nltk \n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy.linalg import cholesky\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmFAw4uiDsyi",
    "outputId": "ab617a7d-7279-41d5-a9b3-59794e2df371"
   },
   "outputs": [],
   "source": [
    "%cd /Users/prathudharbagathi/Downloads/Github/ML/KDD-NPA-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3hbAOZx6rFoC"
   },
   "outputs": [],
   "source": [
    "behaviour_train_file = 'behaviors.tsv'\n",
    "news_train_file = 'news.tsv'\n",
    "\n",
    "# behaviour_dev_file = '/content/drive/My Drive/MINDsmall_dev/behaviors.tsv'\n",
    "# news_dev_file = '/content/drive/My Drive/MINDsmall_dev/news.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JYjzMlDvFa3g"
   },
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LgWlqyXjaABh",
    "outputId": "d30fceb7-0512-4f29-8650-59a620795a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20772 37536\n"
     ]
    }
   ],
   "source": [
    "################# Reading news data \n",
    "# max_sentence_len = 0\n",
    "sent_len_for_model = 30\n",
    "with open(news_train_file) as news_f:\n",
    "  news_k = news_f.readlines()\n",
    "\n",
    "news_dict = {}\n",
    "content ={}\n",
    "word_dict_raw={'PADDING':[0,999999]}\n",
    "\n",
    "for doc in news_k:\n",
    "  d = doc.strip().split('\\t')\n",
    "  newsid = d[0]\n",
    "  ## Format is id,category,sub-category,Title,Abstract,link,Wikidata/metadata\n",
    "  if newsid not in news_dict:\n",
    "    news_dict[newsid] = len(news_dict)\n",
    "    tokenized_news = word_tokenize(d[3].lower())\n",
    "    # max_sentence_len = max(max_sentence_len,len(tokenized_news))\n",
    "    content[news_dict[newsid]] = [d[1],d[2],tokenized_news]\n",
    "    for word in tokenized_news:\n",
    "        if word in word_dict_raw:\n",
    "            word_dict_raw[word][1]+=1\n",
    "        else:\n",
    "            word_dict_raw[word]=[len(word_dict_raw),1]\n",
    "word_dict={}\n",
    "for i in word_dict_raw:\n",
    "    if word_dict_raw[i][1]>=2:\n",
    "        word_dict[i]=[len(word_dict),word_dict_raw[i][1]]\n",
    "print(len(word_dict),len(word_dict_raw))\n",
    "##### words for each news\n",
    "news_words=[]\n",
    "news_index={}\n",
    "for newsid in content:\n",
    "    word_id=[]\n",
    "    news_dict[newsid]=len(news_index)\n",
    "    for word in content[newsid][2]:\n",
    "        if word in word_dict:\n",
    "            word_id.append(word_dict[word][0])\n",
    "    word_id=word_id[:30]\n",
    "    news_words.append(word_id+[0]*(30-len(word_id)))\n",
    "news_words=np.array(news_words,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51282, 30)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9vqWWHwqX6g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzF63Xn7Xh-S"
   },
   "source": [
    "Sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "K5zc-PfNYCHf"
   },
   "outputs": [],
   "source": [
    "################# Reading click behavior data \n",
    "min_neg_samples_required = 10\n",
    "npratio = 10\n",
    "#########################\n",
    "with open(behaviour_train_file) as f2:\n",
    "  userdata_new = f2.readlines()\n",
    "\n",
    "userid_dict_total = {}\n",
    "cnt_gr = 0\n",
    "cnt = 0 \n",
    "############################################\n",
    "all_train_id=[]\n",
    "all_train_pn=[]    \n",
    "all_label=[]\n",
    "all_user_pos=[]\n",
    "\n",
    "############################################\n",
    "\n",
    "for user in userdata_new:\n",
    "  cnt +=1\n",
    "  line = user.strip().split('\\t')\n",
    "  userid = line[1]\n",
    "  impre_id = line[0]\n",
    "  if userid not in userid_dict_total:\n",
    "    userid_dict_total[userid] = len(userid_dict_total)\n",
    "  click_history = line[3].strip().split()\n",
    "  new_line = line[4].strip().split()\n",
    "  trainpos=[x.split('-')[0]  for x in new_line if x[-1]=='1']\n",
    "  trainneg=[x.split('-')[0] for x in new_line if x[-1]=='0']\n",
    "  \n",
    "  if len(trainneg) > 0:\n",
    "      for pos_sample in trainpos:\n",
    "          pos_neg_sample = newsample(trainneg,npratio)\n",
    "          pos_neg_sample.append(pos_sample)\n",
    "          temp_label=[0 for i in range(npratio)]+[1]\n",
    "          temp_id=list(range(npratio+1))\n",
    "          random.shuffle(temp_id)\n",
    "\n",
    "          shuffle_sample=[]\n",
    "          shuffle_label=[]\n",
    "          for id in temp_id:\n",
    "              shuffle_sample.append(news_dict[pos_neg_sample[id]])\n",
    "              shuffle_label.append(temp_label[id])\n",
    "          posset=list(set(click_history)-set([pos_sample]))\n",
    "          allpos=[news_dict[p] for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "          allpos+=[0]*(50-len(allpos))\n",
    "          all_train_pn.append(shuffle_sample)\n",
    "          all_label.append(shuffle_label)\n",
    "          all_train_id.append(userid_dict_total[userid])\n",
    "          all_user_pos.append(allpos)\n",
    "############################################\n",
    "all_train_pn_total=np.array(all_train_pn,dtype='int32')\n",
    "all_label_total=np.array(all_label,dtype='int32')\n",
    "all_train_id_total=np.array(all_train_id,dtype='int32')\n",
    "all_user_pos_total=np.array(all_user_pos,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHZtMR6cbCYa",
    "outputId": "3d242dc0-6062-4f1e-ea97-9b83afa5a011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236344, 11)\n",
      "(236344, 11)\n",
      "(236344,)\n",
      "(236344, 50)\n"
     ]
    }
   ],
   "source": [
    "print(all_train_pn_total.shape)\n",
    "print(all_label_total.shape)\n",
    "print(all_train_id_total.shape)\n",
    "print(all_user_pos_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "yGMZyJkyJfKI"
   },
   "outputs": [],
   "source": [
    "unique_users,user_counts = np.unique(all_train_id_total,return_counts=True)\n",
    "selected_user_ids = unique_users[user_counts>15]\n",
    "########################\n",
    "final_id_mask = np.isin(all_train_id_total,selected_user_ids)\n",
    "final_pn_total= all_train_pn_total[final_id_mask,]\n",
    "final_label_total = all_label_total[final_id_mask,]\n",
    "final_id_total = all_train_id_total[final_id_mask]\n",
    "final_user_pos_total = all_user_pos_total[final_id_mask,]\n",
    "###################\n",
    "unique_id,first_occurance = np.unique(np.flip(final_id_total),return_index=True)\n",
    "final_index = len(final_id_total)-1-first_occurance\n",
    "######################\n",
    "all_index = np.array(range(0,len(final_id_total)))\n",
    "\n",
    "test_index = np.isin(all_index,final_index)\n",
    "\n",
    "train_index = ~test_index\n",
    "##########################\n",
    "all_test_id = final_id_total[test_index]\n",
    "all_test_pn = final_pn_total[test_index,]\n",
    "all_test_user_pos = final_user_pos_total[test_index,]\n",
    "all_test_label = final_label_total[test_index,]\n",
    "all_test_index = np.array(range(0,len(all_test_id)))\n",
    "\n",
    "all_train_id = final_id_total[train_index]\n",
    "all_train_pn = final_pn_total[train_index,]\n",
    "all_user_pos = final_user_pos_total[train_index,]\n",
    "all_label = final_label_total[train_index,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivVwZ2NMY5qm",
    "outputId": "0d181aeb-ff23-4cd2-de88-59cad04ca35f"
   },
   "outputs": [],
   "source": [
    "print(all_train_pn.shape)\n",
    "print(all_label.shape)\n",
    "print(all_train_id.shape)\n",
    "print(all_user_pos.shape)\n",
    "print(len(userid_dict_total))\n",
    "print(userid_dict_total.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TINv2t3sbU1e",
    "outputId": "82b2f4f8-fdf9-472d-91d7-5f69be7495cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2336, 11)\n",
      "(2336, 11)\n",
      "(2336,)\n",
      "(2336, 50)\n"
     ]
    }
   ],
   "source": [
    "print(all_test_pn.shape)\n",
    "print(all_test_label.shape)\n",
    "print(all_test_id.shape)\n",
    "print(all_test_user_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict\n",
    "def get_embedding(word_dict):\n",
    "    embedding_dict={}\n",
    "    cnt=0\n",
    "    # with open('/data/wuch/glove.840B.300d.txt','rb')as f:\n",
    "    with open('glove.840B.300d.txt','rb') as f:\n",
    "        linenb=0\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if len(line)==0:\n",
    "                break\n",
    "            line = line.split()\n",
    "            word=line[0].decode()\n",
    "            linenb+=1\n",
    "            if len(word) != 0:\n",
    "                vec=[float(x) for x in line[1:]]\n",
    "                if word in word_dict:\n",
    "                    embedding_dict[word]=vec\n",
    "                    if cnt%1000==0:\n",
    "                        print(cnt,linenb,word)\n",
    "                    cnt+=1\n",
    "\n",
    "    embedding_matrix=[0]*len(word_dict)\n",
    "    cand=[]\n",
    "    for i in embedding_dict:\n",
    "        embedding_matrix[word_dict[i][0]]=np.array(embedding_dict[i],dtype='float32')\n",
    "        cand.append(embedding_matrix[word_dict[i][0]])\n",
    "    cand=np.array(cand,dtype='float32')\n",
    "    mu=np.mean(cand, axis=0)\n",
    "    Sigma=np.cov(cand.T)\n",
    "    norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "    for i in range(len(embedding_matrix)):\n",
    "        if type(embedding_matrix[i])==int:\n",
    "            embedding_matrix[i]=np.reshape(norm, 300)\n",
    "    embedding_matrix[0]=np.zeros(300,dtype='float32')\n",
    "    embedding_matrix=np.array(embedding_matrix,dtype='float32')\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_mat=get_embedding(word_dict)\n",
    "# embedding_mat = torch.FloatTensor(embedding_mat)\n",
    "# torch.save(embedding_mat,'embedding_mat.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_mat = torch.load('embedding_mat.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_SENT_LENGTH = 30\n",
    "MAX_SENTS = 50\n",
    "N = 32 # batch size\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size,max_s):\n",
    "        super(Attention, self).__init__()\n",
    "        self.nb_head = num_heads\n",
    "        self.size_per_head = head_size\n",
    "        self.output_dim = num_heads * head_size\n",
    "        self.ms=max_s\n",
    "\n",
    "        # Define weight matrices\n",
    "        self.WQ = nn.Parameter(torch.empty(self.size_per_head, self.size_per_head))\n",
    "        self.WK = nn.Parameter(torch.empty(self.size_per_head, self.size_per_head))\n",
    "        self.WV = nn.Parameter(torch.empty(self.size_per_head, self.ms))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = torch.zeros_like(inputs)\n",
    "            mask = mask.scatter(2, seq_len.unsqueeze(-1), 1)  # Change 1 to -1 to correctly unsqueeze\n",
    "            mask = 1 - torch.cumsum(mask, 2)  # Change 1 to 2 to correctly calculate cumsum along dim 2\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(len(x))\n",
    "        if len(x) == 3:\n",
    "            Q_seq, K_seq, V_seq = x\n",
    "            Q_len, V_len = None, None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq, K_seq, V_seq, Q_len, V_len = x\n",
    "        \n",
    "        # Linear transformation for queries, keys, and values\n",
    "        Q_seq = torch.matmul(Q_seq, self.WQ)\n",
    "#         print(Q_seq.shape)\n",
    "#         Q_seq = Q_seq.view(-1, self.nb_head, Q_seq.size(2), self.size_per_head)\n",
    "#         print(Q_seq.shape)\n",
    "#         Q_seq = Q_seq.permute(0, 2, 1, 3)\n",
    "        \n",
    "        K_seq = torch.matmul(K_seq, self.WK)\n",
    "#         K_seq = K_seq.view(-1, self.nb_head, K_seq.size(2), self.size_per_head)\n",
    "#         K_seq = K_seq.permute(0, 2, 1, 3)\n",
    "        \n",
    "        V_seq = torch.matmul(V_seq, self.WV)\n",
    "#         V_seq = V_seq.view(-1, self.nb_head, V_seq.size(2), self.size_per_head)\n",
    "#         V_seq = V_seq.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        A = torch.matmul(Q_seq, K_seq.permute(0, 1, 3, 2))\n",
    "        A = A / (self.size_per_head**0.5)\n",
    "        A = A.permute(0, 3, 2, 1)\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = A.permute(0, 3, 2, 1)\n",
    "        \n",
    "        # Apply softmax activation\n",
    "        A = nn.functional.softmax(A, dim=-1)\n",
    "        \n",
    "        # Compute output sequence\n",
    "        O_seq = torch.matmul(A, V_seq)\n",
    "#         print('O_seq',O_seq.shape)\n",
    "#         O_seq = O_seq.contiguous().view(-1, O_seq.size(1), self.output_dim)\n",
    "#         O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        \n",
    "        return O_seq\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][0], input_shape[0][1], self.output_dim\n",
    "\n",
    "\n",
    "\n",
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_sent_length, max_sents, num_heads=16):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_mat = embedding_mat\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.embedding_mat,freeze=False)\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embedding_dim\n",
    "        self.self_attentions = nn.ModuleList()\n",
    "\n",
    "# Create num_heads number of Attention modules and append to self.self_attentions list\n",
    "        for i in range(num_heads):\n",
    "            attention_module = Attention(num_heads, self.head_size,max_sent_length)\n",
    "            self.self_attentions.append(attention_module) # Pass num_heads and head_size\n",
    "            \n",
    "        self.dense = nn.Linear(self.num_heads, 1)\n",
    "        self.dense2 = nn.Linear((npratio+1)*self.num_heads, (npratio+1))\n",
    "        self.projection = nn.Linear(max_sent_length, 1)\n",
    "        self.l1=nn.Linear(self.head_size,1)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        # Projection layer for attention weight\n",
    "        self.bias = nn.Parameter(torch.Tensor(max_sent_length,1))  # Bias term for attention weight\n",
    "        \n",
    "    def forward(self, input):\n",
    "#         print('Input of newsencoder',input.shape)\n",
    "        embedded_sequences = self.embedding(input)\n",
    "#         print('AFTER EMBEDING',embedded_sequences.shape)\n",
    "        self_atts = []\n",
    "#         print(self.num_heads)\n",
    "        for i in range(self.num_heads):\n",
    "            selfatt = self.self_attentions[i]([embedded_sequences, embedded_sequences, embedded_sequences])\n",
    "            self_atts.append(selfatt)\n",
    "        selfatt = torch.cat(self_atts, dim=1)\n",
    "#         print(selfatt.shape)\n",
    "#         print(selfatt.permute(0,2,3,1).shape)\n",
    "#         print(num_heads)\n",
    "        if selfatt.shape[1]==self.num_heads:\n",
    "            attention = self.dense(selfatt.permute(0,2,3,1))\n",
    "        else:\n",
    "            attention = self.dense2(selfatt.permute(0,2,3,1))\n",
    "#         print(attention.shape)\n",
    "        attention_weight = self.softmax(attention)\n",
    "#         print('passed')\n",
    "#         + self.bias.to(input.device)\n",
    "        # Add projection to attention weight\n",
    "#         print(attention_weight.permute(0,3,2,1).shape)\n",
    "#         att_weight_proj = self.projection(attention_weight.permute(0,3,2,1)) \n",
    "        out1=[]\n",
    "        for i in range(embedded_sequences.shape[1]):\n",
    "            out1.append(torch.bmm(attention_weight.permute(0,3,2,1)[:,i,:,:].squeeze(),embedded_sequences[:,i,:,:]))\n",
    "        out1=torch.stack(out1,dim=1)\n",
    "#         print('out1 shapr',out1.shape)\n",
    "        att_weight_proj = torch.tanh(self.l1(out1)+self.bias)\n",
    "        \n",
    "#         print('Attn weight is',att_weight_proj)\n",
    "#         print(att_weight_proj.shape)\n",
    "        # Apply softmax to get new attention weights\n",
    "#         print(att_weight_proj)\n",
    "        rep = nn.Softmax(dim=1)(att_weight_proj)\n",
    "#         print('rep shape',rep.permute(0,1,3,2).shape)\n",
    "        out=[]\n",
    "        for i in range(embedded_sequences.shape[1]):\n",
    "            out.append(torch.bmm(rep.permute(0,1,3,2)[:,i,:,:],out1[:,i,:,:]))\n",
    "        out=torch.stack(out,dim=2)\n",
    "#         print('out shape',out.shape)\n",
    "#         print(rep)\n",
    "        # Apply linear layer for final output\n",
    "#         rep = torch.matmul(selfatt.transpose(1,2), attention_weight_new)\n",
    "#         print('Rep is', rep)\n",
    "        return out\n",
    "#         rep = torch.matmul(selfatt.transpose(1,2), attention_weight)\n",
    "#         return rep\n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_sent_length, max_sents, num_heads=16):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_mat = embedding_mat\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.embedding_mat,freeze=False)\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embedding_dim\n",
    "        self.self_attentions = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            attention_module = Attention(num_heads, self.head_size,max_sents)\n",
    "            self.self_attentions.append(attention_module) # Pass num_heads and head_size\n",
    "        self.dense = nn.Linear(self.num_heads, 1)\n",
    "        self.projection = nn.Linear(max_sents, 1)  # Projection layer for attention weight\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.lu=nn.Linear(self.head_size,1)\n",
    "        self.bias = nn.Parameter(torch.Tensor(max_sents,1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        embedded_sequences = input\n",
    "#         print('UE Input',embedded_sequences.shape)\n",
    "        self_atts = []\n",
    "        for i in range(self.num_heads):\n",
    "            selfatt = self.self_attentions[i]([embedded_sequences, embedded_sequences, embedded_sequences])\n",
    "            self_atts.append(selfatt)\n",
    "        selfatt = torch.cat(self_atts, dim=1)\n",
    "#         print('UE selfatt',selfatt.shape)\n",
    "        attention = self.dense(selfatt.permute(0,2,3,1))\n",
    "        attention_weight = self.softmax(attention)\n",
    "#         print('UE Att',attention_weight.shape)\n",
    "        out1 = torch.bmm(attention_weight.squeeze(),embedded_sequences.squeeze())\n",
    "#         print('UE out1',out1.shape)\n",
    "        att_weight_proj = torch.tanh(self.lu(out1)+self.bias)\n",
    "#         print('UE att_weight_proj',att_weight_proj.shape)\n",
    "        rep = nn.Softmax(dim=1)(att_weight_proj)\n",
    "        out=torch.bmm(rep.permute(0,2,1),out1)\n",
    "#         print('UE out',out.shape)\n",
    "        return out\n",
    "\n",
    "class NewsRecommendationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_sent_length, max_sents, npratio):\n",
    "        super(NewsRecommendationModel, self).__init__()\n",
    "        self.titleEncoder = NewsEncoder(vocab_size, embedding_dim, max_sent_length, max_sents)\n",
    "        self.useEncoder = UserEncoder(vocab_size, embedding_dim, max_sent_length, max_sents)\n",
    "        self.candidateEncoder = NewsEncoder(vocab_size, embedding_dim, max_sent_length, max_sents)\n",
    "        self.dense = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.logits = nn.Linear(embedding_dim, npratio+1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, candidates, news_input):\n",
    "        news_encoders = []\n",
    "        for i in range(MAX_SENTS):\n",
    "#             print(news_input[:, i][:,None].shape)\n",
    "#             print('Input is',news_input[:, i][:,None])\n",
    "            x=torch.tensor(news_input[:, i][:,None]).clone().detach()\n",
    "            news_encoders.append(self.titleEncoder(x))  # Pass news_input[:, i]\n",
    "        news_encoders = torch.stack(news_encoders, dim=1).squeeze(dim=2)\n",
    "#         print('NE',news_encoders.shape)\n",
    "        y=news_encoders.permute(0,2,1,3).clone().detach().requires_grad_(True)\n",
    "        user_encoder= self.useEncoder(y)\n",
    "#         print('NR ue',user_encoder.shape)\n",
    "        candidate_encoder = self.candidateEncoder(torch.tensor(candidates))\n",
    "#         print('NR cand',candidate_encoder.shape)\n",
    "#         print('encoders are',news_encoders[10],news_encoders[11],news_encoders[24])\n",
    "        x = torch.matmul(candidate_encoder.squeeze(),user_encoder.permute(0,2,1)).requires_grad_(True)\n",
    "#         print('NR candidate',candidate_encoder.squeeze())\n",
    "#         print('NR Use encoder',user_encoder.permute(0,2,1))\n",
    "#         print('NR final out',x)\n",
    "        x=nn.Softmax(dim=1)(x)\n",
    "#         print('NR final out',x)\n",
    "        return x\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_random(batch_size):\n",
    "    idlist = np.arange(len(all_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=all_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "#     while (True):\n",
    "    for i in batches:\n",
    "        item = news_words[all_train_pn[i]]\n",
    "        user=news_words[all_user_pos[i]]\n",
    "        yield ([item,user], [y[i]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1335.3228\n",
      "Epoch [2/5], Loss: 1337.3611\n",
      "Epoch [3/5], Loss: 1335.8109\n",
      "Epoch [4/5], Loss: 1335.1006\n",
      "Epoch [5/5], Loss: 1334.3921\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(word_dict) # replace with appropriate vocabulary size\n",
    "embedding_dim = 300\n",
    "max_sent_length = 30\n",
    "max_sents = 50\n",
    "npratio = 10 # number of negative samples for each positive sample\n",
    "lr = 0.1 # learning rate\n",
    "epochs = 5\n",
    "full_batch=64\n",
    "batches = 100 # mini-batch size\n",
    "# class_freq=[1,5]\n",
    "# class_weights = torch.FloatTensor(class_freq)\n",
    "# Create instances of the model, loss function, and optimizer\n",
    "model = NewsRecommendationModel(vocab_size, embedding_dim, max_sent_length, max_sents, npratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data\n",
    "    train_data = generate_batch_data_random(batches)\n",
    "#     print('passed')\n",
    "    # Mini-batch training\n",
    "    i = 0\n",
    "    total_loss=0\n",
    "    while True:\n",
    "        try:\n",
    "            # Prepare mini-batch data\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_data = next(train_data)\n",
    "            batch_candidates = batch_data[0][0]\n",
    "            batch_news_input = batch_data[0][1]\n",
    "            batch_label = torch.tensor(batch_data[1][0])\n",
    "            \n",
    "#             print(batch_news_input.shape)\n",
    "           \n",
    "            logits = model(batch_candidates, batch_news_input).squeeze()\n",
    "            \n",
    "#             print('After modeling')\n",
    "#             print(logits,batch_label)\n",
    "            \n",
    "            loss = criterion(logits, batch_label.float())\n",
    "            loss.backward()\n",
    "           \n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            total_loss+=loss.detach()\n",
    "            \n",
    "\n",
    "            if i >= 10000:  # Change num_iterations to the desired number\n",
    "                raise StopIteration\n",
    "            \n",
    "            i+=1\n",
    "        except StopIteration:\n",
    "            # End of generator, break out of the loop\n",
    "            break\n",
    "        \n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch+1, epochs, total_loss))\n",
    "    \n",
    "# Training complete\n",
    "print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path='/Users/prathudharbagathi/Downloads/Course work/Spring/NLP/Project/KDD-NPA-/model.pt'\n",
    "# file_path2='/Users/prathudharbagathi/Downloads/Course work/Spring/NLP/Project/KDD-NPA-/modelx.pt'\n",
    "# file_path2='/Users/prathudharbagathi/Downloads/Course work/Spring/NLP/Project/KDD-NPA-/model3.pt'\n",
    "# file_path2='/Users/prathudharbagathi/Downloads/Course work/Spring/NLP/Project/KDD-NPA-/model4.pt'\n",
    "file_path2='/Users/prathudharbagathi/Downloads/Course work/Spring/NLP/Project/KDD-NPA-/modelfinal.pt'\n",
    "torch.save(model, file_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "#     print(np.sum(y_true))\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_test_id = final_id_total[test_index]\n",
    "# all_test_pn = final_pn_total[test_index,]\n",
    "# all_test_user_pos = final_user_pos_total[test_index,]\n",
    "# all_test_label = final_label_total[test_index,]\n",
    "# all_test_index = np.array(range(0,len(all_test_id)))\n",
    "\n",
    "def generate_batch_data_randomt(batch_size):\n",
    "    idlist = np.arange(len(all_test_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=all_test_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "#     while (True):\n",
    "    for i in batches:\n",
    "        item = news_words[all_test_pn[i]]\n",
    "        user=news_words[all_test_user_pos[i]]\n",
    "        yield ([item,user], [y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare test data\n",
    "test_data = generate_batch_data_randomt(batches)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for batch_data in test_data:\n",
    "    batch_candidates = batch_data[0][0]\n",
    "    batch_news_input = batch_data[0][1]\n",
    "    batch_label = batch_data[1][0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_candidates, batch_news_input).squeeze()\n",
    "        pred = torch.sigmoid(logits).round().long()\n",
    "    \n",
    "    y_pred.append(pred)\n",
    "    y_true.append(batch_label)\n",
    "\n",
    "# Convert predictions to a numpy array\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "# Convert true labels to a numpy array\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Print classification report\n",
    "# print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr=[]\n",
    "ndcgt=[]\n",
    "ndcg5=[]\n",
    "auc=[]\n",
    "for i in range(len(y_true)):\n",
    "    yt=y_true[i,:]\n",
    "    yp=y_pred[i,:]\n",
    "    mrr.append(mrr_score(yt,yp ))\n",
    "    ndcg5.append(ndcg_score(yt,yp,5))\n",
    "    ndcgt.append(ndcg_score(yt,yp,10))\n",
    "    auc.append(roc_auc_score(yt,yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mrr))\n",
    "print(np.mean(ndcg5))\n",
    "print(np.mean(ndcgt))\n",
    "print(np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=64\n",
    "\n",
    "# idlist = np.arange(len(all_label))\n",
    "# np.random.shuffle(idlist)\n",
    "# y=all_label\n",
    "# j=0\n",
    "# batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "# for i in batches:\n",
    "#     item = news_words[all_train_pn[i]]\n",
    "#     user=news_words[all_user_pos[i]]\n",
    "# #         yield ([item,user], [y[i]])\n",
    "#     j+=1\n",
    "# #     print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size=64\n",
    "# idlist = np.arange(len(all_label))\n",
    "# np.random.shuffle(idlist)\n",
    "# y=all_label\n",
    "# batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "# len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor(all_user_pos_shuffled[:,2][:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data\n",
    "    train_data = generate_batch_data_random(32)\n",
    "    # Mini-batch training\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Prepare mini-batch data\n",
    "            batch_data = next(train_data)\n",
    "            batch_candidates = batch_data[0][0]\n",
    "            batch_news_input = batch_data[0][1]\n",
    "            batch_label = batch_data[1]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            logits = model(batch_candidates, batch_news_input)\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, batch_label)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Print batch loss\n",
    "            print(\"Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}\".format(epoch+1, epochs, i+1, len(train_data), loss.item()))\n",
    "            i += 1\n",
    "        except StopIteration:\n",
    "            # End of generator, break out of the loop\n",
    "            break\n",
    "\n",
    "# Training complete\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model architecture\n",
    "class ClickPredictionModel(nn.Module):\n",
    "    def __init__(self, num_users, num_news_articles, max_sent_length, max_sents, word_dict_size, embedding_dim):\n",
    "        super(ClickPredictionModel, self).__init__()\n",
    "        \n",
    "        self.title_input = nn.Embedding(word_dict_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.news_input = nn.Sequential(\n",
    "            nn.Linear(max_sents * max_sent_length * embedding_dim, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.candidates = nn.Sequential(\n",
    "            nn.Linear(max_sent_length * embedding_dim, 200),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(200, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.user_rep = nn.Linear(200, 200)\n",
    "        \n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(200, 1 + npratio),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, candidates, news_input):\n",
    "        batch_size = candidates.size(0)\n",
    "        num_candidates = candidates.size(1)\n",
    "        \n",
    "        # Embed candidate news articles\n",
    "        candidates = candidates.view(batch_size * num_candidates, -1)\n",
    "        candidate_vecs = self.candidates(candidates)\n",
    "        \n",
    "        # Embed news input\n",
    "        news_input = news_input.view(batch_size, -1)\n",
    "        news_encoders = self.news_input(news_input)\n",
    "        \n",
    "        # Compute attention weights for news input\n",
    "        news_attention = self.attention(news_encoders)\n",
    "        \n",
    "        # Compute user representation\n",
    "        user_rep = self.user_rep(torch.matmul(news_encoders.permute(0,2,1), news_attention))\n",
    "        \n",
    "        # Compute logits for click prediction\n",
    "        logits = self.logits(torch.matmul(user_rep, candidate_vecs.permute(0,2,1)))\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create an instance of the model\n",
    "model = ClickPredictionModel(num_users, num_news_articles, MAX_SENT_LENGTH, MAX_SENTS, len(word_dict), 300)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Convert input data to PyTorch tensors\n",
    "candidates_tensor = torch.tensor(all_train_pn, dtype=torch.long)\n",
    "news_input_tensor = torch.tensor(all_user_pos, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(all_label, dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(candidates_tensor, news_input_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(logits, labels_tensor.view(-1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([32, 1, 200, 1])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Define your linear layer\n",
    "# linear_layer = nn.Linear(30, 1)\n",
    "\n",
    "# # Create a sample input tensor\n",
    "# input_tensor = torch.randn(32, 1, 200, 30)\n",
    "\n",
    "# # Reshape the input tensor to [32, 1, 200, 30] -> [32, 1, 200, 30]\n",
    "# input_tensor = input_tensor.view(32, 1, 200, 30)\n",
    "\n",
    "# # Flatten the input tensor to [32, 1, 200, 30] -> [32, 1, 200, 6000]\n",
    "# input_tensor = input_tensor.view(32, 1, 200, -1)\n",
    "\n",
    "# # Pass the flattened input tensor through the linear layer\n",
    "# output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "# # Reshape the output tensor to [32, 1, 200, 6000] -> [32, 1, 200]\n",
    "# # output_tensor = output_tensor.view(32, 1, 200)\n",
    "\n",
    "# # Print the shape of the output tensor\n",
    "# print(\"Output tensor shape:\", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
